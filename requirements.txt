# Core ML
# For CPU-based inference with Ollama, we don't need torch, transformers, accelerate, bitsandbytes
# torch
# transformers
# accelerate
# bitsandbytes
ollama>=0.1.8

# Video Processing
opencv-python>=4.8.0
ffmpeg-python>=0.2.0
Pillow>=10.0.0

# Web Framework
fastapi>=0.104.0
uvicorn>=0.24.0
streamlit>=1.28.0
python-multipart>=0.0.6

# Agent Framework (LangChain not used in this simplified version)
# langchain
# langchain-experimental

# Utilities
pydantic>=2.0.0
numpy>=1.24.0
requests>=2.31.0
python-dotenv>=0.21.0
